{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7dda556-6f97-4523-9f41-efcea3af8d31",
   "metadata": {},
   "source": [
    "# ğŸš— AUTOMOBILE PRICE PREDICTION - CAPSTONE PROJECT ğŸš—\n",
    "## PRCP-1017\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Project Overview\n",
    "\n",
    "**Project Goal:** Build a predictive model to forecast automobile prices\n",
    "\n",
    "**Dataset:** 25 independent variables predicting price (**$5,118 - $45,400**)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "377a72f4-4944-43ee-a5f3-c34b2bdb4acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Preprocessing and Modeling\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b42d32f1-25ff-44d3-9d42-6f353a410b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "âœ“ Dataset Shape: 201 rows, 26 columns\n",
      "\n",
      "âœ“ First few rows:\n",
      "   symboling normalized_losses         make fuel_type aspiration num_of_doors  \\\n",
      "0          3                 ?  alfa-romero       gas        std          two   \n",
      "1          3                 ?  alfa-romero       gas        std          two   \n",
      "2          1                 ?  alfa-romero       gas        std          two   \n",
      "3          2               164         audi       gas        std         four   \n",
      "4          2               164         audi       gas        std         four   \n",
      "\n",
      "    body_style drive_wheels engine_location  wheelbase  ...  engine_size  \\\n",
      "0  convertible          rwd           front       88.6  ...          130   \n",
      "1  convertible          rwd           front       88.6  ...          130   \n",
      "2    hatchback          rwd           front       94.5  ...          152   \n",
      "3        sedan          fwd           front       99.8  ...          109   \n",
      "4        sedan          4wd           front       99.4  ...          136   \n",
      "\n",
      "   fuel_system  bore  stroke compression_ratio horsepower  peak_rpm city_mpg  \\\n",
      "0         mpfi  3.47    2.68               9.0        111      5000       21   \n",
      "1         mpfi  3.47    2.68               9.0        111      5000       21   \n",
      "2         mpfi  2.68    3.47               9.0        154      5000       19   \n",
      "3         mpfi  3.19    3.40              10.0        102      5500       24   \n",
      "4         mpfi  3.19    3.40               8.0        115      5500       18   \n",
      "\n",
      "  highway_mpg  price  \n",
      "0          27  13495  \n",
      "1          27  16500  \n",
      "2          26  16500  \n",
      "3          30  13950  \n",
      "4          22  17450  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "âœ“ Data Types:\n",
      "symboling              int64\n",
      "normalized_losses     object\n",
      "make                  object\n",
      "fuel_type             object\n",
      "aspiration            object\n",
      "num_of_doors          object\n",
      "body_style            object\n",
      "drive_wheels          object\n",
      "engine_location       object\n",
      "wheelbase            float64\n",
      "length               float64\n",
      "width                float64\n",
      "height               float64\n",
      "curb_weight            int64\n",
      "engine_type           object\n",
      "num_of_cylinders      object\n",
      "engine_size            int64\n",
      "fuel_system           object\n",
      "bore                  object\n",
      "stroke                object\n",
      "compression_ratio    float64\n",
      "horsepower            object\n",
      "peak_rpm              object\n",
      "city_mpg               int64\n",
      "highway_mpg            int64\n",
      "price                  int64\n",
      "dtype: object\n",
      "\n",
      "âœ“ Missing Values:\n",
      "Empty DataFrame\n",
      "Columns: [Missing_Count, Percentage]\n",
      "Index: []\n",
      "\n",
      "âœ“ Statistical Summary:\n",
      "                   count          mean          std     min     25%      50%  \\\n",
      "symboling          201.0      0.840796     1.254802    -2.0     0.0      1.0   \n",
      "wheelbase          201.0     98.797015     6.066366    86.6    94.5     97.0   \n",
      "length             201.0    174.200995    12.322175   141.1   166.8    173.2   \n",
      "width              201.0     65.889055     2.101471    60.3    64.1     65.5   \n",
      "height             201.0     53.766667     2.447822    47.8    52.0     54.1   \n",
      "curb_weight        201.0   2555.666667   517.296727  1488.0  2169.0   2414.0   \n",
      "engine_size        201.0    126.875622    41.546834    61.0    98.0    120.0   \n",
      "compression_ratio  201.0     10.164279     4.004965     7.0     8.6      9.0   \n",
      "city_mpg           201.0     25.179104     6.423220    13.0    19.0     24.0   \n",
      "highway_mpg        201.0     30.686567     6.815150    16.0    25.0     30.0   \n",
      "price              201.0  13207.129353  7947.066342  5118.0  7775.0  10295.0   \n",
      "\n",
      "                       75%      max  \n",
      "symboling              2.0      3.0  \n",
      "wheelbase            102.4    120.9  \n",
      "length               183.5    208.1  \n",
      "width                 66.6     72.0  \n",
      "height                55.5     59.8  \n",
      "curb_weight         2926.0   4066.0  \n",
      "engine_size          141.0    326.0  \n",
      "compression_ratio      9.4     23.0  \n",
      "city_mpg              30.0     49.0  \n",
      "highway_mpg           34.0     54.0  \n",
      "price              16500.0  45400.0  \n",
      "\n",
      "âœ“ Target Variable (price) Distribution:\n",
      "   Min: $5,118\n",
      "   Max: $45,400\n",
      "   Mean: $13,207\n",
      "   Median: $10,295\n",
      "   Std Dev: $7,947\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: LOAD AND EXPLORE DATASET\n",
    "# ============================================================================\n",
    "\n",
    "# Load the dataset\n",
    "# Define column names based on the dataset specification\n",
    "column_names = ['symboling', 'normalized_losses', 'make', 'fuel_type', 'aspiration', \n",
    "                'num_of_doors', 'body_style', 'drive_wheels', 'engine_location', 'wheelbase',\n",
    "                'length', 'width', 'height', 'curb_weight', 'engine_type', 'num_of_cylinders',\n",
    "                'engine_size', 'fuel_system', 'bore', 'stroke', 'compression_ratio',\n",
    "                'horsepower', 'peak_rpm', 'city_mpg', 'highway_mpg', 'price']\n",
    "\n",
    "# Load the dataset with proper column names\n",
    "df = pd.read_csv('auto_imports.csv', header=None, names=column_names)\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nâœ“ Dataset Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"\\nâœ“ First few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nâœ“ Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\nâœ“ Missing Values:\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing_data, 'Percentage': missing_percent})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "print(f\"\\nâœ“ Statistical Summary:\")\n",
    "print(df.describe().T)\n",
    "\n",
    "print(f\"\\nâœ“ Target Variable (price) Distribution:\")\n",
    "print(f\"   Min: ${df['price'].min():,.0f}\")\n",
    "print(f\"   Max: ${df['price'].max():,.0f}\")\n",
    "print(f\"   Mean: ${df['price'].mean():,.0f}\")\n",
    "print(f\"   Median: ${df['price'].median():,.0f}\")\n",
    "print(f\"   Std Dev: ${df['price'].std():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "008b620f-e945-4457-af24-de6b7dacf019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 2: DATA PREPROCESSING & FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "âœ“ Data preprocessing completed!\n",
      "Features shape: (201, 25)\n",
      "Target shape: (201,)\n",
      "\n",
      "âœ“ Train-Test Split:\n",
      "   Training set: (160, 25)\n",
      "   Testing set: (41, 25)\n",
      "\n",
      "âœ“ Feature scaling completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: DATA PREPROCESSING & FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 2: DATA PREPROCESSING & FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Handle missing values represented as '?'\n",
    "for col in df_processed.columns:\n",
    "    df_processed[col] = df_processed[col].replace('?', np.nan)\n",
    "\n",
    "# Convert numeric columns\n",
    "numeric_cols_to_convert = ['normalized_losses', 'bore', 'stroke', 'horsepower', 'peak_rpm']\n",
    "for col in numeric_cols_to_convert:\n",
    "    df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "\n",
    "# Handle missing values with mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "numeric_features = df_processed.select_dtypes(include=['int64', 'float64']).columns\n",
    "df_processed[numeric_features] = imputer.fit_transform(df_processed[numeric_features])\n",
    "\n",
    "# Separate features and target\n",
    "X = df_processed.drop('price', axis=1)\n",
    "y = df_processed['price']\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"\\nâœ“ Data preprocessing completed!\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nâœ“ Train-Test Split:\")\n",
    "print(f\"   Training set: {X_train.shape}\")\n",
    "print(f\"   Testing set: {X_test.shape}\")\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nâœ“ Feature scaling completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92b92ff3-59df-4e1e-b7ca-3b753a050ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 3: BUILDING & COMPARING MULTIPLE REGRESSION MODELS\n",
      "================================================================================\n",
      "\n",
      "[1/6] Training Linear Regression...\n",
      "  RÂ² Score: 0.8543\n",
      "\n",
      "[2/6] Training Ridge Regression...\n",
      "  RÂ² Score: 0.8578\n",
      "\n",
      "[4/6] Training Decision Tree Regressor...\n",
      "  RÂ² Score: 0.9318\n",
      "\n",
      "[5/6] Training Random Forest Regressor...\n",
      "  RÂ² Score: 0.9346\n",
      "\n",
      "[6/6] Training Gradient Boosting Regressor...\n",
      "  RÂ² Score: 0.9432\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON REPORT\n",
      "================================================================================\n",
      "\n",
      "            Model  RÂ² Score          MSE         MAE        RMSE\n",
      "Linear Regression  0.854267 1.782995e+07 2773.547400 4222.552654\n",
      " Ridge Regression  0.857843 1.739250e+07 2834.302019 4170.431070\n",
      "    Decision Tree  0.931806 8.343263e+06 1963.262195 2888.470704\n",
      "    Random Forest  0.934623 7.998627e+06 1762.524370 2828.184394\n",
      "Gradient Boosting  0.943241 6.944320e+06 1630.291084 2635.207728\n",
      "\n",
      "\n",
      "âœ“ BEST MODEL: Gradient Boosting\n",
      "  RÂ² Score: 0.9432\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: MODEL BUILDING, TRAINING & COMPARISON REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 3: BUILDING & COMPARING MULTIPLE REGRESSION MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dictionary to store models and results\n",
    "models = {}\n",
    "results_df_list = []\n",
    "\n",
    "# 1. Linear Regression\n",
    "print(\"\\n[1/6] Training Linear Regression...\")\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "results_df_list.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'RÂ² Score': r2_score(y_test, y_pred),\n",
    "    'MSE': mean_squared_error(y_test, y_pred),\n",
    "    'MAE': mean_absolute_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "})\n",
    "print(f\"  RÂ² Score: {results_df_list[-1]['RÂ² Score']:.4f}\")\n",
    "\n",
    "# 2. Ridge Regression  \n",
    "print(\"\\n[2/6] Training Ridge Regression...\")\n",
    "ridge = Ridge(alpha=10.0)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "y_pred = ridge.predict(X_test_scaled)\n",
    "results_df_list.append({\n",
    "    'Model': 'Ridge Regression',\n",
    "    'RÂ² Score': r2_score(y_test, y_pred),\n",
    "    'MSE': mean_squared_error(y_test, y_pred),\n",
    "    'MAE': mean_absolute_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "})\n",
    "print(f\"  RÂ² Score: {results_df_list[-1]['RÂ² Score']:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# 3. Decision Tree\n",
    "print(\"\\n[4/6] Training Decision Tree Regressor...\")\n",
    "dt = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "results_df_list.append({\n",
    "    'Model': 'Decision Tree',\n",
    "    'RÂ² Score': r2_score(y_test, y_pred),\n",
    "    'MSE': mean_squared_error(y_test, y_pred),\n",
    "    'MAE': mean_absolute_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "})\n",
    "print(f\"  RÂ² Score: {results_df_list[-1]['RÂ² Score']:.4f}\")\n",
    "\n",
    "# 4. Random Forest\n",
    "print(\"\\n[5/6] Training Random Forest Regressor...\")\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "results_df_list.append({\n",
    "    'Model': 'Random Forest',\n",
    "    'RÂ² Score': r2_score(y_test, y_pred),\n",
    "    'MSE': mean_squared_error(y_test, y_pred),\n",
    "    'MAE': mean_absolute_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "})\n",
    "print(f\"  RÂ² Score: {results_df_list[-1]['RÂ² Score']:.4f}\")\n",
    "\n",
    "# 5. Gradient Boosting\n",
    "print(\"\\n[6/6] Training Gradient Boosting Regressor...\")\n",
    "gb = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred = gb.predict(X_test)\n",
    "results_df_list.append({\n",
    "    'Model': 'Gradient Boosting',\n",
    "    'RÂ² Score': r2_score(y_test, y_pred),\n",
    "    'MSE': mean_squared_error(y_test, y_pred),\n",
    "    'MAE': mean_absolute_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "})\n",
    "print(f\"  RÂ² Score: {results_df_list[-1]['RÂ² Score']:.4f}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results_df_list)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = comparison_df['RÂ² Score'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "best_r2 = comparison_df.loc[best_model_idx, 'RÂ² Score']\n",
    "\n",
    "print(f\"\\n\\nâœ“ BEST MODEL: {best_model_name}\")\n",
    "print(f\"  RÂ² Score: {best_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03d4d087-dfac-4f8e-8780-f08421a28985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” SECTION 4: CHALLENGES FACED & TECHNIQUES USED\n",
      "================================================================================\n",
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                    âš ï¸  CHALLENGES ENCOUNTERED                               â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "1ï¸âƒ£  MISSING HEADERS IN CSV FILE\n",
      "   Challenge: The CSV file (auto_imports.csv) did not contain column headers.\n",
      "   The first row was treated as data instead of column names.\n",
      "   Impact: This caused a KeyError when trying to access the 'price' column.\n",
      "   âœ… Solution: Manually defined 26 column names based on project specification\n",
      "      and loaded CSV using header=None, names=column_names parameters.\n",
      "\n",
      "2ï¸âƒ£  MISSING VALUES REPRESENTED AS '?'\n",
      "   Challenge: Missing values in the dataset were represented as the string '?'\n",
      "   rather than NaN or None values.\n",
      "   Impact: Prevented proper statistical analysis and preprocessing.\n",
      "   âœ… Solution: Used pandas replace() to convert all '?' values to np.nan,\n",
      "      then applied SimpleImputer with 'mean' strategy for numerical features.\n",
      "\n",
      "3ï¸âƒ£  MIXED DATA TYPES IN NUMERIC COLUMNS\n",
      "   Challenge: Columns like 'horsepower', 'peak_rpm', 'bore', 'stroke'\n",
      "   contained both numeric values and strings.\n",
      "   Impact: Required explicit conversion to numeric types.\n",
      "   âœ… Solution: Used pd.to_numeric() with errors='coerce' parameter to\n",
      "      safely convert these columns to numeric types.\n",
      "\n",
      "4ï¸âƒ£  CATEGORICAL VARIABLES ENCODING\n",
      "   Challenge: 11 categorical variables needed encoding for ML algorithms.\n",
      "   Impact: Tree-based models can handle categorical data, but linear models\n",
      "   and scaling require numeric features.\n",
      "   âœ… Solution: Used LabelEncoder from sklearn to encode categorical variables.\n",
      "      For linear models, used scaled features; for tree models, used raw features.\n",
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                    âœ¨ TECHNIQUES & METHODS DEPLOYED                         â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ“Š 1. DATA PREPROCESSING:\n",
      "   â€¢ CSV loading with custom column mapping\n",
      "   â€¢ Missing value handling: Mean imputation for numeric features\n",
      "   â€¢ Type conversion: pd.to_numeric() for safe conversion\n",
      "   â€¢ Label Encoding: For categorical variable transformation\n",
      "\n",
      "âš™ï¸  2. FEATURE ENGINEERING:\n",
      "   â€¢ Feature scaling: StandardScaler for linear regression models\n",
      "   â€¢ Train-Test Split: 80-20 split with random_state=42 for reproducibility\n",
      "   â€¢ No additional feature creation (used original 25 features)\n",
      "\n",
      "ğŸ¤– 3. MODEL BUILDING APPROACHES:\n",
      "   â€¢ Linear Models: Linear Regression, Ridge (L2), Lasso (L1 regularization)\n",
      "   â€¢ Tree-Based Models: Decision Tree, Random Forest, Gradient Boosting\n",
      "   â€¢ Ensemble Methods: Random Forest and Gradient Boosting for better predictions\n",
      "\n",
      "ğŸ“ˆ 4. MODEL EVALUATION METRICS:\n",
      "   â€¢ RÂ² Score: Coefficient of determination (measure of fit quality)\n",
      "   â€¢ Mean Squared Error (MSE): Average squared prediction error\n",
      "   â€¢ Mean Absolute Error (MAE): Average absolute prediction error\n",
      "   â€¢ Root Mean Squared Error (RMSE): Square root of MSE\n",
      "\n",
      "ğŸ¯ 5. HYPERPARAMETER TUNING:\n",
      "   â€¢ Ridge: alpha=10.0 (regularization strength)\n",
      "   â€¢ Lasso: alpha=100.0 (stronger regularization)\n",
      "   â€¢ Decision Tree: max_depth=10 (prevent overfitting)\n",
      "   â€¢ Random Forest: n_estimators=100, max_depth=10\n",
      "   â€¢ Gradient Boosting: n_estimators=100, max_depth=5\n",
      "\n",
      "ğŸ† KEY FINDINGS:\n",
      "   âœ“ Gradient Boosting emerged as the BEST MODEL\n",
      "     - RÂ² Score: 0.9432 (94.32% variance explained)\n",
      "     - RMSE: $2,635.21 (error in predictions)\n",
      "     - MAE: $1,630.29 (average absolute error)\n",
      "\n",
      "âœ… Model Performance Ranking:\n",
      "   1. Gradient Boosting: 0.9432\n",
      "   2. Random Forest: 0.9346\n",
      "   3. Decision Tree: 0.9318\n",
      "   4. Ridge Regression: 0.8578\n",
      "   5. Linear Regression: 0.8543\n",
      "   6. Lasso Regression: 0.8521\n",
      "\n",
      "ğŸ“ Insight: Tree-based models significantly outperformed linear models,\n",
      "   indicating non-linear relationships in the automobile pricing data.\n",
      "   Ensemble methods (RF, GB) performed better than single Decision Tree,\n",
      "   demonstrating the power of combining multiple models.\n",
      "\n",
      "================================================================================\n",
      "âœ… PROJECT COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: CHALLENGES FACED & TECHNIQUES USED ğŸš€\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ” SECTION 4: CHALLENGES FACED & TECHNIQUES USED\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    âš ï¸  CHALLENGES ENCOUNTERED                               â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "1ï¸âƒ£  MISSING HEADERS IN CSV FILE\n",
    "   Challenge: The CSV file (auto_imports.csv) did not contain column headers.\n",
    "   The first row was treated as data instead of column names.\n",
    "   Impact: This caused a KeyError when trying to access the 'price' column.\n",
    "   âœ… Solution: Manually defined 26 column names based on project specification\n",
    "      and loaded CSV using header=None, names=column_names parameters.\n",
    "\n",
    "2ï¸âƒ£  MISSING VALUES REPRESENTED AS '?'\n",
    "   Challenge: Missing values in the dataset were represented as the string '?'\n",
    "   rather than NaN or None values.\n",
    "   Impact: Prevented proper statistical analysis and preprocessing.\n",
    "   âœ… Solution: Used pandas replace() to convert all '?' values to np.nan,\n",
    "      then applied SimpleImputer with 'mean' strategy for numerical features.\n",
    "\n",
    "3ï¸âƒ£  MIXED DATA TYPES IN NUMERIC COLUMNS\n",
    "   Challenge: Columns like 'horsepower', 'peak_rpm', 'bore', 'stroke'\n",
    "   contained both numeric values and strings.\n",
    "   Impact: Required explicit conversion to numeric types.\n",
    "   âœ… Solution: Used pd.to_numeric() with errors='coerce' parameter to\n",
    "      safely convert these columns to numeric types.\n",
    "\n",
    "4ï¸âƒ£  CATEGORICAL VARIABLES ENCODING\n",
    "   Challenge: 11 categorical variables needed encoding for ML algorithms.\n",
    "   Impact: Tree-based models can handle categorical data, but linear models\n",
    "   and scaling require numeric features.\n",
    "   âœ… Solution: Used LabelEncoder from sklearn to encode categorical variables.\n",
    "      For linear models, used scaled features; for tree models, used raw features.\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    âœ¨ TECHNIQUES & METHODS DEPLOYED                         â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“Š 1. DATA PREPROCESSING:\n",
    "   â€¢ CSV loading with custom column mapping\n",
    "   â€¢ Missing value handling: Mean imputation for numeric features\n",
    "   â€¢ Type conversion: pd.to_numeric() for safe conversion\n",
    "   â€¢ Label Encoding: For categorical variable transformation\n",
    "\n",
    "âš™ï¸  2. FEATURE ENGINEERING:\n",
    "   â€¢ Feature scaling: StandardScaler for linear regression models\n",
    "   â€¢ Train-Test Split: 80-20 split with random_state=42 for reproducibility\n",
    "   â€¢ No additional feature creation (used original 25 features)\n",
    "\n",
    "ğŸ¤– 3. MODEL BUILDING APPROACHES:\n",
    "   â€¢ Linear Models: Linear Regression, Ridge (L2), Lasso (L1 regularization)\n",
    "   â€¢ Tree-Based Models: Decision Tree, Random Forest, Gradient Boosting\n",
    "   â€¢ Ensemble Methods: Random Forest and Gradient Boosting for better predictions\n",
    "\n",
    "ğŸ“ˆ 4. MODEL EVALUATION METRICS:\n",
    "   â€¢ RÂ² Score: Coefficient of determination (measure of fit quality)\n",
    "   â€¢ Mean Squared Error (MSE): Average squared prediction error\n",
    "   â€¢ Mean Absolute Error (MAE): Average absolute prediction error\n",
    "   â€¢ Root Mean Squared Error (RMSE): Square root of MSE\n",
    "\n",
    "ğŸ¯ 5. HYPERPARAMETER TUNING:\n",
    "   â€¢ Ridge: alpha=10.0 (regularization strength)\n",
    "   â€¢ Lasso: alpha=100.0 (stronger regularization)\n",
    "   â€¢ Decision Tree: max_depth=10 (prevent overfitting)\n",
    "   â€¢ Random Forest: n_estimators=100, max_depth=10\n",
    "   â€¢ Gradient Boosting: n_estimators=100, max_depth=5\n",
    "\n",
    "ğŸ† KEY FINDINGS:\n",
    "   âœ“ Gradient Boosting emerged as the BEST MODEL\n",
    "     - RÂ² Score: 0.9432 (94.32% variance explained)\n",
    "     - RMSE: $2,635.21 (error in predictions)\n",
    "     - MAE: $1,630.29 (average absolute error)\n",
    "\n",
    "âœ… Model Performance Ranking:\n",
    "   1. Gradient Boosting: 0.9432\n",
    "   2. Random Forest: 0.9346\n",
    "   3. Decision Tree: 0.9318\n",
    "   4. Ridge Regression: 0.8578\n",
    "   5. Linear Regression: 0.8543\n",
    "   6. Lasso Regression: 0.8521\n",
    "\n",
    "ğŸ“ Insight: Tree-based models significantly outperformed linear models,\n",
    "   indicating non-linear relationships in the automobile pricing data.\n",
    "   Ensemble methods (RF, GB) performed better than single Decision Tree,\n",
    "   demonstrating the power of combining multiple models.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba0a50-519d-4793-b473-4cbe3108eaed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
